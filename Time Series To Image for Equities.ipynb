{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import quandl\n",
    "import ta\n",
    "import finta\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_directory = os.path.join(os.getcwd(), 'data','train')\n",
    "test_directory = os.path.join(os.getcwd(), 'data','test')\n",
    "ordered_directory = os.path.join(os.getcwd(), 'data','ordered')\n",
    "WINDOW_LOOK_AHEAD = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equites to train models for\n",
    "equity_list = ['MMM','AXP','AAPL','BA','CAT','CVX','CSCO','KO','DIS','XOM','GE','GS','HD','IBM','INTC','JNJ','JPM','MCD','MRK','MSFT','NKE','PFE','PG','TRV','UTX','UNH','VZ','WMT']\n",
    "# equity_list = ['MMM','AXP','AAPL','BA']  #Short version for debugging\n",
    "# equity_list = ['AAPL']  #Short version for debugging\n",
    "feature_list = ['RSI','Williams','WMA','EMA','SMA','HMA','3EMA','CCI','CMO','MACD','PPO','ROC','CMFI','DMI','PSI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each equity you chose above, downloads the data and computes the financial indicators\n",
    "def get_eq_dict(eq_list):\n",
    "    equities_dfs = {}\n",
    "    for equity in eq_list:\n",
    "#         print(equity)\n",
    "        output_df = pd.DataFrame()\n",
    "        #Add your quandl key below\n",
    "        temp_df = quandl.get(\"EOD/\"+equity, authtoken=\"YOUR KEY HERE\")\n",
    "        date_range_df = temp_df.loc['2001-11-01':'2017-2-28']\n",
    "        data_range_df_finta = date_range_df.copy() \n",
    "        data_range_df_finta.columns = data_range_df_finta.columns.str.lower()\n",
    "        output_df['RSI'] = ta.momentum.rsi(date_range_df['Close'])\n",
    "        output_df['Williams'] = ta.momentum.wr(date_range_df['High'],date_range_df['Low'],date_range_df['Close'])\n",
    "        output_df['WMA'] = ta.trend.wma_indicator(date_range_df['Close'])\n",
    "        output_df['EMA'] = ta.trend.ema_indicator(date_range_df['Close'])\n",
    "        output_df['SMA'] = ta.trend.sma_indicator(date_range_df['Close'])\n",
    "        output_df['HMA'] =  finta.TA.HMA(data_range_df_finta[['open','high','low','close']])\n",
    "        output_df['3EMA'] = ta.trend.trix(date_range_df['Close'])  #Tripple EMA\n",
    "        output_df['CCI'] = ta.trend.cci(date_range_df['High'],date_range_df['Low'],date_range_df['Close'])  \n",
    "        output_df['CMO'] =  finta.TA.CMO(data_range_df_finta[['open','high','low','close']])\n",
    "        output_df['MACD'] = ta.trend.macd(date_range_df['Close'])  \n",
    "        output_df['PPO'] = ta.momentum.PercentagePriceOscillator(date_range_df['Close']).ppo()\n",
    "        output_df['ROC'] = ta.momentum.ROCIndicator(date_range_df['Close']).roc()\n",
    "        output_df['CMFI'] = ta.volume.ChaikinMoneyFlowIndicator(date_range_df['High'],date_range_df['Low'],date_range_df['Close'], date_range_df['Volume']).chaikin_money_flow()\n",
    "        output_df['DMI'] =  ta.trend.ADXIndicator(date_range_df['High'],date_range_df['Low'],date_range_df['Close']).adx() # ADX is average direction movement index\n",
    "        output_df['PSI'] = ta.trend.PSARIndicator(date_range_df['High'],date_range_df['Low'],date_range_df['Close']).psar()\n",
    "        output_df['Close'] = date_range_df['Close']\n",
    "        output_df['Label'] = 'hold'\n",
    "        output_df = output_df.reset_index()\n",
    "        equities_dfs[equity] = output_df\n",
    "    return equities_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scales all features to be between 0 and 1\n",
    "def scale_and_get_scalers(df_dict,eq_list, features, scaler = None):\n",
    "    if scaler == None:\n",
    "        scalers = {}\n",
    "        for eq in eq_list:\n",
    "            scalers[eq] = MinMaxScaler()\n",
    "            scalers[eq].fit(df_dict[eq][features])\n",
    "            df_dict[eq][features] = scalers[eq].transform(df_dict[eq][features])\n",
    "        return scalers\n",
    "    else:\n",
    "        scalers = scaler\n",
    "        for eq in eq_list:\n",
    "            df_dict[eq][features] = scalers[eq].transform(df_dict[eq][features])\n",
    "        return scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lables all days buy, hold or sell. See paper in repository for more details.\n",
    "def label_data(df_dict,eq_list):\n",
    "    for eq in eq_list:\n",
    "        for i in range(len(df_dict[eq])):\n",
    "            # I went 2 months beyond at each end so we could have data for all dates\n",
    "            if i > 6 and i+5 < len(df_dict[eq]):\n",
    "                buy = True\n",
    "                sell = True\n",
    "                for j in range(1,6):\n",
    "                    if df_dict[eq].loc[i-j,'Close'] <= df_dict[eq].loc[i,'Close'] or df_dict[eq].loc[i+j,'Close'] <= df_dict[eq].loc[i,'Close']:\n",
    "                        sell = False\n",
    "                    if df_dict[eq].loc[i-j,'Close'] >= df_dict[eq].loc[i,'Close'] or df_dict[eq].loc[i+j,'Close'] >= df_dict[eq].loc[i,'Close']:\n",
    "                        buy = False\n",
    "                if buy == True:\n",
    "                    df_dict[eq].loc[i,'Label'] = 'buy'\n",
    "                elif sell == True:\n",
    "                    df_dict[eq].loc[i,'Label'] = 'sell'\n",
    "                else:\n",
    "                    df_dict[eq].loc[i,'Label'] = 'hold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each equity, generates the training data for this time window in datadir.  Ordered_dir is for keeping the\n",
    "#  dates in order for predictions.  Otherwise the dates are sorted by the labels.\n",
    "def gen_training_data(df_dict,eq_list,start, end, features, data_dir, test = False, ordered_dir=None):\n",
    "    if os.path.exists(data_dir):\n",
    "        shutil.rmtree(data_dir)\n",
    "    os.makedirs(data_dir)\n",
    "    for eq in eq_list:\n",
    "        os.makedirs(os.path.join(data_dir,eq))\n",
    "        os.makedirs(os.path.join(data_dir,eq,'buy'))\n",
    "        os.makedirs(os.path.join(data_dir,eq,'hold'))\n",
    "        os.makedirs(os.path.join(data_dir,eq,'sell'))\n",
    "\n",
    "    \n",
    "    if test == True:\n",
    "        if os.path.exists(ordered_dir):\n",
    "            shutil.rmtree(ordered_dir)\n",
    "        os.makedirs(ordered_dir)\n",
    "        for eq in eq_list:\n",
    "            os.makedirs(os.path.join(ordered_dir,eq))\n",
    "            os.makedirs(os.path.join(ordered_dir,eq,'hold'))\n",
    "        \n",
    "    for eq in eq_list:\n",
    "        train_df = df_dict[eq][(df_dict[eq]['Date']>=(pd.to_datetime(start)- pd.DateOffset(days=25)))&(df_dict[eq]['Date']<=end)]\n",
    "        train_df.sort_values(by=['Date'])\n",
    "        train_df.reset_index(drop=True, inplace = True)\n",
    "        train_df.fillna(0)\n",
    "        far_enough = False\n",
    "        first_row = train_df[train_df.Date >= start].index[0]\n",
    "        for i in range(first_row,len(train_df)):\n",
    "            np_im = train_df.loc[i-14:i,features].to_numpy()\n",
    "            img = Image.fromarray(np.uint8(np_im*255))\n",
    "            if train_df.loc[i,'Label'] == 'buy':\n",
    "                img.save(os.path.join(data_dir,eq,'buy', str(eq) + str(i)  +'.png'))                    \n",
    "            elif train_df.loc[i,'Label'] == 'sell':\n",
    "                img.save(os.path.join(data_dir,eq,'sell', str(eq) + str(i)  +'.png'))                    \n",
    "            else:\n",
    "                img.save(os.path.join(data_dir,eq,'hold', str(eq) + str(i)  +'.png')) \n",
    "            if test == True:\n",
    "                img.save(os.path.join(ordered_dir,eq,'hold', str(eq) + str(i)  +'.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to weight the loss function\n",
    "def get_num_per_class(dataset):\n",
    "    labels = torch.zeros(3)\n",
    "    for _, target in dataset:\n",
    "#         print(target)\n",
    "        labels[target] += 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Called in train model, all of the data loaders are built here for each time window\n",
    "def get_loaders(equity_name,batch_size=128):\n",
    "    \n",
    "    ## Use start and end date to generate the data here?\n",
    "    \n",
    "    data_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.25, 0.25, 0.25]) ])\n",
    "    train_data_dir = os.path.join(os.getcwd(), 'data','train',equity_name)\n",
    "    test_data_dir = os.path.join(os.getcwd(), 'data','test',equity_name)\n",
    "    train_data = datasets.ImageFolder(train_data_dir, data_transform)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle = True)\n",
    "    test_data = datasets.ImageFolder(test_data_dir, data_transform)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=300, shuffle=False, num_workers=4)\n",
    "    \n",
    "    ordered_data_dir = os.path.join(os.getcwd(), 'data','ordered',equity_name)\n",
    "    test_data_ordered = datasets.ImageFolder(ordered_data_dir, data_transform)\n",
    "    test_dataloader_ordered = torch.utils.data.DataLoader(test_data_ordered, batch_size=300, shuffle=False, num_workers=4)\n",
    "\n",
    "    \n",
    "    \n",
    "    train_dataset_size = len(train_data)\n",
    "    test_dataset_size = len(test_data)\n",
    "    ordered_dataset_size = len(test_data_ordered)\n",
    "    \n",
    "    class_names = train_data.classes\n",
    "    return train_dataloader, test_dataloader, train_dataset_size, test_dataset_size, train_data, test_dataloader_ordered,ordered_dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, ReLU, Sequential, Conv2d, MaxPool2d, Softmax, BatchNorm2d, Dropout, Module, CrossEntropyLoss, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN-Tar model.  See paper for architecture\n",
    "class Basic_CNN(Module):   \n",
    "    def __init__(self,linear_size=64*7*7):\n",
    "        super(Basic_CNN, self).__init__()\n",
    "        self.layer1 = Sequential(\n",
    "            Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            ReLU(inplace=True),\n",
    "            BatchNorm2d(32))\n",
    "        self.layer2 =  Sequential(\n",
    "            Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            ReLU(inplace=True),\n",
    "            BatchNorm2d(64),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            Flatten()\n",
    "        )\n",
    "            \n",
    "        self.Dropout1 = Dropout(0.25)\n",
    "        \n",
    "        self.Dropout2 = Dropout(0.5)\n",
    "\n",
    "        self.linear_layer1 = Linear(linear_size, 128)\n",
    "        self.linear_layer2 = Linear(128, 3)\n",
    "        self.soft_m = Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.Dropout1(x)\n",
    "        x = self.linear_layer1(x)\n",
    "        x = self.Dropout2(x)\n",
    "        x = self.linear_layer2(x)\n",
    "        x = self.soft_m(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom inception block used for the inception experiments.  If changing the number of features change the linear_size as well\n",
    "class Inception(Module):   \n",
    "    def __init__(self,linear_size=173475):\n",
    "        super(Inception, self).__init__()\n",
    "        self.CONV1 = Sequential(Conv2d(3, 64, kernel_size=1),ReLU(inplace=True),BatchNorm2d(64))\n",
    "        self.CONV2 = Sequential(Conv2d(3, 64, kernel_size=3, padding=1),ReLU(inplace=True),BatchNorm2d(64))\n",
    "        self.CONV3 = Sequential(Conv2d(3, 64, kernel_size=5, padding=2),ReLU(inplace=True),BatchNorm2d(64))\n",
    "        self.CONV4 = Sequential(Conv2d(3, 64, kernel_size=7, padding=3),ReLU(inplace=True),BatchNorm2d(64))\n",
    "        self.MP1 = Sequential(MaxPool2d(kernel_size = 3,stride=1,padding=1))\n",
    "        \n",
    "        self.CONV5 = Sequential(Conv2d(259, 128, kernel_size=1),ReLU(inplace=True),BatchNorm2d(128))\n",
    "        self.CONV6 = Sequential(Conv2d(259, 128, kernel_size=3, padding=1),ReLU(inplace=True),BatchNorm2d(128))\n",
    "        self.CONV7 = Sequential(Conv2d(259, 128, kernel_size=5, padding=2),ReLU(inplace=True),BatchNorm2d(128))\n",
    "        self.CONV8 =Sequential( Conv2d(259, 128, kernel_size=7, padding=3),ReLU(inplace=True),BatchNorm2d(128))\n",
    "        self.MP2 = Sequential(MaxPool2d(kernel_size = 3,stride=1,padding=1))\n",
    "        self.Flat = Sequential( Flatten())\n",
    "        self.Dropout1 = Dropout(0.25)\n",
    "        \n",
    "        self.Dropout2 = Dropout(0.5)\n",
    "\n",
    "        self.linear_layer1 = Linear(linear_size, 128)\n",
    "        self.linear_layer2 = Linear(128, 3)\n",
    "        self.soft_m = Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.CONV1(x)\n",
    "        x2 = self.CONV2(x)\n",
    "        x3 = self.CONV3(x)\n",
    "        x4 = self.CONV4(x)\n",
    "        m1 = self.MP1(x)\n",
    "                              \n",
    "        x = torch.cat([x1,x2,x3,x4,m1], 1)\n",
    "                              \n",
    "        x5 = self.CONV5(x)\n",
    "        x6 = self.CONV6(x)\n",
    "        x7 = self.CONV7(x)\n",
    "        x8 = self.CONV8(x)\n",
    "        m2 = self.MP1(x)\n",
    "                              \n",
    "        x = torch.cat([x5,x6,x7,x8,m2], 1)\n",
    "                              \n",
    "        x = self.Flat(x)\n",
    "        print(x.shape)\n",
    "        x = self.Dropout1(x)\n",
    "        x = self.linear_layer1(x)\n",
    "        x = self.Dropout2(x)\n",
    "        x = self.linear_layer2(x)\n",
    "        x = self.soft_m(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized the model, criterion and loss fucniton for the baseline and additional data experiments\n",
    "def initialize(train_data, linear_size=(64*7*7)):\n",
    "    model = Basic_CNN(linear_size).cuda()\n",
    "    class_weights = 1/ get_num_per_class(train_data).cuda()\n",
    "    class_weights[0] *=2.0\n",
    "    class_weights[2] *=2.0\n",
    "    criterion = CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized the model, criterion and loss fucniton for the inception experiments\n",
    "def initialize_inception(train_data, linear_size=(173475)):\n",
    "    model = Inception(linear_size).cuda()\n",
    "    class_weights = 1/ get_num_per_class(train_data).cuda()\n",
    "    class_weights[0] *=1.15\n",
    "    class_weights[2] *=1.15\n",
    "    criterion = CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I swapped the columns to match the order in the paper so it's easier to compare\n",
    "def PrintConfMatrix(data):\n",
    "    print('                       Predicted')\n",
    "    print('                  Hold       Buy      Sell')\n",
    "    print('         Hold  ' ,data[1,1].item() ,' ' ,data[1,0].item() ,'  ',data[1,2].item())\n",
    "    print('Actual   Buy   ' ,data[0,1].item() ,'  ' ,data[0,0].item() ,'  ',data[0,2].item())\n",
    "    print('         Sell  ' ,data[2,1].item() ,'  ' ,data[2,0].item() ,'   ',data[2,2].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reporting recall, precision and F1 in the order I wanted them\n",
    "def eval_mat(data):\n",
    "    print('                   Hold                  Buy                    Sell')\n",
    "    Hrecall = data[1,1].item()/(data[1,1].item() + data[1,0].item() +data[1,2].item())\n",
    "    Brecall = data[0,0].item()/(data[0,0].item() + data[0,1].item() + data[0,2].item())\n",
    "    Srecall = data[2,2].item()/(data[2,2].item()+data[2,0].item()+data[2,1].item())\n",
    "    \n",
    "    Hprec = data[1,1].item()/(data[1,1].item()+data[0,1].item() +data[2,1].item())\n",
    "    Bprec = data[0,0].item()/(data[0,0].item()+data[1,0].item() +data[2,0].item())\n",
    "    Sprec = data[2,2].item()/(data[2,2].item()+data[0,2].item() +data[1,2].item())\n",
    "    \n",
    "    \n",
    "    print('Recall      ' ,Hrecall,'  '  ,Brecall ,'   ',Srecall)\n",
    "    print('Precision   ', Hprec ,'  ' ,Bprec ,'  ',Sprec)\n",
    "    print('F1 Score    ', 2*(Hrecall*Hprec)/(Hrecall+Hprec) ,'  ' ,2*(Brecall*Bprec)/(Brecall+Bprec) ,'   ',2*(Srecall*Sprec)/(Srecall+Sprec))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains on one time window of 5 years.  Called for all models\n",
    "def train_model(equity, old_model = None,linear_size=(64*7*7), inception = False, batch_size = 128):\n",
    "    # Train the model\n",
    "    print(equity)\n",
    "    if inception == True:\n",
    "        linear_size = 173475\n",
    "    \n",
    "    train_dataloader, test_dataloader, train_dataset_size, test_dataset_size, train_data, test_dataloader_ordered,ordered_dataset_size = get_loaders(equity,batch_size)\n",
    "    if inception == False:\n",
    "        model, criterion, optimizer = initialize(train_data, linear_size)\n",
    "    else:\n",
    "        model, criterion, optimizer = initialize_inception(train_data, linear_size)\n",
    "    if old_model != None:\n",
    "        model = old_model\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    total_step = len(train_dataloader)\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        start_time = time.time()\n",
    "        for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.cuda())\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels.cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.cuda().data)\n",
    "\n",
    "        epoch_loss = running_loss / train_dataset_size\n",
    "        epoch_acc = running_corrects.double() / train_dataset_size\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        #This was outputting more info than we needed.  I'm leaving it here in case someone wants it for testing\n",
    "#         print('Epoch: {} Loss: {:.4f} Acc: {:.4f}   Time: {:.4f}'.format(\n",
    "#             epoch, epoch_loss, epoch_acc,time.time()-start_time))   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, test_dataloader, test_dataloader_ordered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests 1 year(or whatever you set it to) window following the training period\n",
    "def test_model(model, test_dataloader):\n",
    "    nb_classes = 3\n",
    "    model.eval()\n",
    "    predictions = None\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            if predictions == None:\n",
    "                predictions = preds\n",
    "            else:\n",
    "                torch.cat((predictions, preds))\n",
    "            for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "#     PrintConfMatrix(confusion_matrix)\n",
    "    return model, confusion_matrix, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_list = ['MMM','AXP','AAPL','BA','CAT','CVX','CSCO','KO','DIS','XOM','GE','GS','HD','IBM','INTC','JNJ','JPM','MCD','MRK','MSFT','NKE','PFE','PG','TRV','UTX','UNH','VZ','WMT']\n",
    "# equity_list = ['MMM','AXP','AAPL','BA']  #Short version for debugging\n",
    "# equity_list = ['AAPL']  #Short version for debugging\n",
    "feature_list = ['RSI','Williams','WMA','EMA','SMA','HMA','3EMA','CCI','CMO','MACD','PPO','ROC','CMFI','DMI','PSI']\n",
    "\n",
    "# These calls build the dataframe, scale it and get the labels\n",
    "equities_dfs = get_eq_dict(equity_list)\n",
    "scaler = scale_and_get_scalers(equities_dfs,equity_list,feature_list)\n",
    "label_data(equities_dfs,equity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "MMM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-6b1359d73ff0>:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.soft_m(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "1\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "2\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "3\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "4\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "5\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "6\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "7\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "8\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "9\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n"
     ]
    }
   ],
   "source": [
    "total_confusion_matrix = torch.zeros(3, 3)\n",
    "start_date = pd.to_datetime('2002-01-01')\n",
    "end_date = pd.to_datetime('2006-12-31')\n",
    "model_dict_base = {}\n",
    "output_dict = {}\n",
    "for equity in equity_list:\n",
    "    output_dict[equity] = []\n",
    "    model_dict_base[equity] = []\n",
    "# 10 sliding windows\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    start = start_date +pd.offsets.DateOffset(years=i)\n",
    "    end = end_date + pd.offsets.DateOffset(years=i)\n",
    "    test_start = start_date +pd.offsets.DateOffset(years=(i+5))\n",
    "    test_end = end_date +pd.offsets.DateOffset(years=(i+1))\n",
    "    # These generate the training data for all equites.  See the folder structure generated if needed\n",
    "    gen_training_data(equities_dfs,equity_list,start,end,feature_list,training_directory)\n",
    "    gen_training_data(equities_dfs,equity_list,test_start,test_end,feature_list,test_directory, test = True, ordered_dir = ordered_directory)\n",
    "    # For each equity in this time window\n",
    "    for equity in equity_list:\n",
    "        # This is here if it predicts all of one class.  The imbalanced data caused this to happen a small amount of the time\n",
    "        tries = 1\n",
    "        while tries <= 10:\n",
    "            tries += 1\n",
    "            model, test_dataloader, test_dataloader_ordered = train_model(equity,batch_size=batch_size)\n",
    "            _, confusion_matrix, _ = test_model(model, test_dataloader)\n",
    "            \n",
    "            _, _, predictions_ord = test_model(model, test_dataloader_ordered)\n",
    "            \n",
    "            if torch.max(torch.sum(confusion_matrix, dim = 0)).item() != torch.sum(confusion_matrix).item():\n",
    "                total_confusion_matrix = torch.add(total_confusion_matrix, confusion_matrix)\n",
    "                break\n",
    "            #If it didn't get it after 10 move on.  Generally gets it second try though\n",
    "            if tries == 11:\n",
    "                total_confusion_matrix = torch.add(total_confusion_matrix, confusion_matrix)\n",
    "        output_dict[equity].append([test_start,predictions_ord])\n",
    "        model_dict_base[equity].append([test_start,model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Predicted\n",
      "                  Hold       Buy      Sell\n",
      "         Hold   23693.0   21588.0    16622.0\n",
      "Actual   Buy    504.0    3672.0    78.0\n",
      "         Sell   558.0    133.0     3656.0\n"
     ]
    }
   ],
   "source": [
    "PrintConfMatrix(total_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Hold                  Buy                    Sell\n",
      "Recall       0.38274397040531155    0.8631875881523272     0.8410397975615367\n",
      "Precision    0.957099575843264    0.14460678139644784    0.1796030654352525\n",
      "F1 Score     0.5468162200835468    0.24771477721185955     0.2959964376796341\n"
     ]
    }
   ],
   "source": [
    "eval_mat(total_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-6b1359d73ff0>:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.soft_m(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#Get the predicitons for each date\n",
    "start_date = pd.to_datetime('2002-01-01')\n",
    "end_date = pd.to_datetime('2006-12-31')\n",
    "output_dict = {}\n",
    "for equity in equity_list:\n",
    "    output_dict[equity] = []\n",
    "    \n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    start = start_date +pd.offsets.DateOffset(years=i)\n",
    "    end = end_date + pd.offsets.DateOffset(years=i)\n",
    "    test_start = start_date +pd.offsets.DateOffset(years=(i+5))\n",
    "    test_end = end_date +pd.offsets.DateOffset(years=(i+1))\n",
    "    gen_training_data(equities_dfs,equity_list,start,end,feature_list,training_directory)\n",
    "    gen_training_data(equities_dfs,equity_list,test_start,test_end,feature_list,test_directory, test = True, ordered_dir = ordered_directory)    \n",
    "    \n",
    "    for equity in equity_list:\n",
    "        _, _, _, _, _, test_dataloader_ordered,_ = get_loaders(equity)\n",
    "        _, _, predictions_ord = test_model(model_dict_base[equity][i][1], test_dataloader_ordered)\n",
    "        output_dict[equity].append([model_dict_base[equity][i][0],predictions_ord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV for analysis\n",
    "for equity in equity_list:\n",
    "    eq_df = pd.DataFrame()\n",
    "    for preds in output_dict[equity]:\n",
    "        if eq_df.empty == True:\n",
    "            eq_df = pd.DataFrame({str(preds[0]):preds[1].tolist()})\n",
    "        else:\n",
    "            temp_df = pd.DataFrame({str(preds[0]):preds[1].tolist()})\n",
    "            eq_df = pd.concat([eq_df, temp_df],axis = 1)\n",
    "    eq_df.to_csv(equity+'_baseline.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_list = ['MMM','AXP','AAPL','BA','CAT','CVX','CSCO','KO','DIS','XOM','GE','GS','HD','IBM','INTC','JNJ','JPM','MCD','MRK','MSFT','NKE','PFE','PG','TRV','UTX','UNH','VZ','WMT']\n",
    "# equity_list = ['MMM']\n",
    "feature_list = ['RSI','Williams','WMA','EMA','SMA','HMA','3EMA','CCI','CMO','MACD','PPO','ROC','CMFI','DMI','PSI','SSMA','EFI','CFI','QSTICK','EVWMA','VFI','FVE','STC','MOM','SAR','VAMA','PERCENT_B','FISH','ER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buildes the eq dict for the new and original features\n",
    "def get_eq_dict_additional_ft(eq_list):\n",
    "    equities_dfs = {}\n",
    "    for equity in eq_list:\n",
    "#         print(equity)\n",
    "        output_df = pd.DataFrame()\n",
    "        temp_df = quandl.get(\"EOD/\"+equity, authtoken=\"6y4QKxqZxio2nBP3VSwZ\")\n",
    "        date_range_df = temp_df.loc['2001-11-01':'2018-2-28']\n",
    "        data_range_df_finta = date_range_df.copy() \n",
    "        data_range_df_finta.columns = data_range_df_finta.columns.str.lower()\n",
    "        output_df['RSI'] = ta.momentum.rsi(date_range_df['Close'])\n",
    "        output_df['Williams'] = ta.momentum.wr(date_range_df['High'],date_range_df['Low'],date_range_df['Close'])\n",
    "        output_df['WMA'] = ta.trend.wma_indicator(date_range_df['Close'])\n",
    "        output_df['EMA'] = ta.trend.ema_indicator(date_range_df['Close'])\n",
    "        output_df['SMA'] = ta.trend.sma_indicator(date_range_df['Close'])\n",
    "        output_df['HMA'] =  finta.TA.HMA(data_range_df_finta[['open','high','low','close']])\n",
    "        output_df['3EMA'] = ta.trend.trix(date_range_df['Close'])  #Tripple EMA\n",
    "        output_df['CCI'] = ta.trend.cci(date_range_df['High'],date_range_df['Low'],date_range_df['Close'])  \n",
    "        output_df['CMO'] =  finta.TA.CMO(data_range_df_finta[['open','high','low','close']])\n",
    "        output_df['MACD'] = ta.trend.macd(date_range_df['Close'])  \n",
    "        output_df['PPO'] = ta.momentum.PercentagePriceOscillator(date_range_df['Close']).ppo()\n",
    "        output_df['ROC'] = ta.momentum.ROCIndicator(date_range_df['Close']).roc()\n",
    "        output_df['CMFI'] = ta.volume.ChaikinMoneyFlowIndicator(date_range_df['High'],date_range_df['Low'],date_range_df['Close'], date_range_df['Volume']).chaikin_money_flow()\n",
    "        output_df['DMI'] =  ta.trend.ADXIndicator(date_range_df['High'],date_range_df['Low'],date_range_df['Close']).adx() # ADX is average direction movement index\n",
    "        output_df['PSI'] = ta.trend.PSARIndicator(date_range_df['High'],date_range_df['Low'],date_range_df['Close']).psar()\n",
    "        \n",
    "        #New Features\n",
    "        output_df['SSMA'] = finta.TA.SSMA(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['EFI'] = finta.TA.EFI(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['CFI'] = finta.TA.CFI(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['QSTICK'] = finta.TA.QSTICK(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['EVWMA'] =  finta.TA.EVWMA(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['VFI'] = finta.TA.VFI(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['FVE'] = finta.TA.FVE(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['STC'] = finta.TA.STC(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['MOM'] = finta.TA.MOM(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['SAR'] = finta.TA.SAR(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['VAMA'] = finta.TA.VAMA(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['PERCENT_B'] = finta.TA.PERCENT_B(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['FISH'] = finta.TA.FISH(data_range_df_finta[['open','high','low','close','volume']])\n",
    "        output_df['ER'] = finta.TA.ER(data_range_df_finta[['open','high','low','close','volume']])\n",
    "\n",
    "        \n",
    "        output_df['Close'] = date_range_df['Close']\n",
    "        output_df['Label'] = 'hold'\n",
    "        output_df = output_df.reset_index()\n",
    "        equities_dfs[equity] = output_df\n",
    "    return equities_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will\\anaconda3\\lib\\site-packages\\ta\\trend.py:643: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i]/self._trs[i])\n",
      "C:\\Users\\Will\\anaconda3\\lib\\site-packages\\ta\\trend.py:647: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i]/self._trs[i])\n"
     ]
    }
   ],
   "source": [
    "#Everything after this point functions the same as the baseline model\n",
    "equities_dfs = get_eq_dict_additional_ft(equity_list)\n",
    "scaler = scale_and_get_scalers(equities_dfs,equity_list,feature_list)\n",
    "label_data(equities_dfs,equity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "MMM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-6b1359d73ff0>:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.soft_m(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AXP\n",
      "AXP\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "XOM\n",
      "GE\n",
      "GE\n",
      "GS\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "PG\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "UNH\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "1\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MCD\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "2\n",
      "MMM\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "MSFT\n",
      "NKE\n",
      "NKE\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "3\n",
      "MMM\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "4\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "PG\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "5\n",
      "MMM\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "HD\n",
      "IBM\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "6\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UTX\n",
      "UTX\n",
      "UTX\n",
      "UTX\n",
      "UTX\n",
      "UNH\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "7\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "MSFT\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UTX\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "8\n",
      "MMM\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "MSFT\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "TRV\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "UNH\n",
      "VZ\n",
      "WMT\n",
      "9\n",
      "MMM\n",
      "AXP\n",
      "AXP\n",
      "AAPL\n",
      "BA\n",
      "CAT\n",
      "CVX\n",
      "CSCO\n",
      "KO\n",
      "KO\n",
      "KO\n",
      "KO\n",
      "DIS\n",
      "XOM\n",
      "GE\n",
      "GS\n",
      "HD\n",
      "IBM\n",
      "INTC\n",
      "JNJ\n",
      "JPM\n",
      "MCD\n",
      "MRK\n",
      "MSFT\n",
      "NKE\n",
      "PFE\n",
      "PG\n",
      "PG\n",
      "TRV\n",
      "UTX\n",
      "UNH\n",
      "UNH\n",
      "VZ\n",
      "WMT\n"
     ]
    }
   ],
   "source": [
    "total_confusion_matrix = torch.zeros(3, 3)\n",
    "start_date = pd.to_datetime('2002-01-01')\n",
    "end_date = pd.to_datetime('2006-12-31')\n",
    "model_dict_ad = {}\n",
    "output_dict_ad = {}\n",
    "for equity in equity_list:\n",
    "    output_dict_ad[equity] = []\n",
    "    model_dict_ad[equity] = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    start = start_date +pd.offsets.DateOffset(years=i)\n",
    "    end = end_date + pd.offsets.DateOffset(years=i)\n",
    "    test_start = start_date +pd.offsets.DateOffset(years=(i+5))\n",
    "    test_end = end_date +pd.offsets.DateOffset(years=(i+1))\n",
    "    gen_training_data(equities_dfs,equity_list,start,end,feature_list,training_directory)\n",
    "    gen_training_data(equities_dfs,equity_list,test_start,test_end,feature_list,test_directory, test = True, ordered_dir = ordered_directory)\n",
    "    for equity in equity_list:\n",
    "        tries = 1\n",
    "        while tries <= 10:\n",
    "            tries += 1\n",
    "            model, test_dataloader, test_dataloader_ordered = train_model(equity,linear_size=(64*7*14))\n",
    "            _, confusion_matrix, _ = test_model(model, test_dataloader)\n",
    "            _, _, predictions_ord = test_model(model, test_dataloader_ordered)\n",
    "            if torch.max(torch.sum(confusion_matrix, dim = 0)).item() != torch.sum(confusion_matrix).item():\n",
    "                total_confusion_matrix = torch.add(total_confusion_matrix, confusion_matrix)\n",
    "                break\n",
    "            if tries == 11:\n",
    "                total_confusion_matrix = torch.add(total_confusion_matrix, confusion_matrix)\n",
    "        output_dict_ad[equity].append([test_start,predictions_ord])\n",
    "        model_dict_ad[equity].append([test_start,model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Predicted\n",
      "                  Hold       Buy      Sell\n",
      "         Hold   26796.0   19205.0    15902.0\n",
      "Actual   Buy    714.0    3436.0    104.0\n",
      "         Sell   755.0    149.0     3443.0\n"
     ]
    }
   ],
   "source": [
    "PrintConfMatrix(total_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Hold                  Buy                    Sell\n",
      "Recall       0.43287078170686394    0.8077103902209685     0.7920404876926616\n",
      "Precision    0.9480275959667434    0.15076788064940763    0.17702709650881793\n",
      "F1 Score     0.5943571998935321    0.2541044224227185     0.2893763657757606\n"
     ]
    }
   ],
   "source": [
    "eval_mat(total_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-6b1359d73ff0>:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.soft_m(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "start_date = pd.to_datetime('2002-01-01')\n",
    "end_date = pd.to_datetime('2006-12-31')\n",
    "output_dict_ad = {}\n",
    "for equity in equity_list:\n",
    "    output_dict_ad[equity] = []\n",
    "    \n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    start = start_date +pd.offsets.DateOffset(years=i)\n",
    "    end = end_date + pd.offsets.DateOffset(years=i)\n",
    "    test_start = start_date +pd.offsets.DateOffset(years=(i+5))\n",
    "    test_end = end_date +pd.offsets.DateOffset(years=(i+1))\n",
    "    gen_training_data(equities_dfs,equity_list,start,end,feature_list,training_directory)\n",
    "    gen_training_data(equities_dfs,equity_list,test_start,test_end,feature_list,test_directory, test = True, ordered_dir = ordered_directory)    \n",
    "    \n",
    "    for equity in equity_list:\n",
    "        _, _, _, _, _, test_dataloader_ordered,_ = get_loaders(equity)\n",
    "        _, _, predictions_ord = test_model(model_dict_ad[equity][i][1], test_dataloader_ordered)\n",
    "        output_dict_ad[equity].append([model_dict_ad[equity][i][0],predictions_ord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for equity in equity_list:\n",
    "    eq_df = pd.DataFrame()\n",
    "    for preds in output_dict_ad[equity]:\n",
    "        if eq_df.empty == True:\n",
    "            eq_df = pd.DataFrame({str(preds[0]):preds[1].tolist()})\n",
    "        else:\n",
    "            temp_df = pd.DataFrame({str(preds[0]):preds[1].tolist()})\n",
    "            eq_df = pd.concat([eq_df, temp_df],axis = 1)\n",
    "    eq_df.to_csv(equity+'_additional.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INCEPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will\\anaconda3\\lib\\site-packages\\ta\\trend.py:643: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i]/self._trs[i])\n",
      "C:\\Users\\Will\\anaconda3\\lib\\site-packages\\ta\\trend.py:647: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i]/self._trs[i])\n"
     ]
    }
   ],
   "source": [
    "# equity_list = ['MMM','AXP','AAPL','BA','CAT','CVX','CSCO','KO','DIS','XOM','GE','GS','HD','IBM','INTC','JNJ','JPM','MCD','MRK','MSFT','NKE','PFE','PG','TRV','UTX','UNH','VZ','WMT']\n",
    "equity_list = ['MMM','AXP','AAPL','BA','CAT','CVX','CSCO']\n",
    "\n",
    "\n",
    "# equity_list = ['MMM','AXP','AAPL','BA']  #Short version for debugging\n",
    "# equity_list = ['AAPL']  #Short version for debugging\n",
    "feature_list = ['RSI','Williams','WMA','EMA','SMA','HMA','3EMA','CCI','CMO','MACD','PPO','ROC','CMFI','DMI','PSI']\n",
    "\n",
    "equities_dfs = get_eq_dict(equity_list)\n",
    "scaler = scale_and_get_scalers(equities_dfs,equity_list,feature_list)\n",
    "label_data(equities_dfs,equity_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_classes = 3\n",
    "batch_size = 512\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_confusion_matrix = torch.zeros(3, 3)\n",
    "start_date = pd.to_datetime('2002-01-01')\n",
    "end_date = pd.to_datetime('2006-12-31')\n",
    "model_dict_in = {}\n",
    "output_dict_in = {}\n",
    "for equity in equity_list:\n",
    "    output_dict_in[equity] = []\n",
    "    model_dict_in[equity] = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    start = start_date +pd.offsets.DateOffset(years=i)\n",
    "    end = end_date + pd.offsets.DateOffset(years=i)\n",
    "    test_start = start_date +pd.offsets.DateOffset(years=(i+5))\n",
    "    test_end = end_date +pd.offsets.DateOffset(years=(i+1))\n",
    "    gen_training_data(equities_dfs,equity_list,start,end,feature_list,training_directory)\n",
    "    gen_training_data(equities_dfs,equity_list,test_start,test_end,feature_list,test_directory, test = True, ordered_dir = ordered_directory)\n",
    "    for equity in equity_list:\n",
    "        tries = 1\n",
    "        while tries <= 10:\n",
    "            tries += 1\n",
    "            #note that inception = True below\n",
    "            model, test_dataloader, test_dataloader_ordered = train_model(equity,inception = True,batch_size = batch_size)\n",
    "            _, confusion_matrix, _ = test_model(model, test_dataloader)\n",
    "            _, _, predictions_ord = test_model(model, test_dataloader_ordered)\n",
    "            # Here I had it retry if any of the labels was never chosen.  Inception was more prone to falling into local minima.\n",
    "            if torch.min(torch.sum(confusion_matrix, dim = 0)).item() != 0.0:\n",
    "                total_confusion_matrix = torch.add(total_confusion_matrix, confusion_matrix)\n",
    "                break\n",
    "            #If it didn't get it after 10 move on.  Generally gets it second try though\n",
    "            if tries == 11:\n",
    "                total_confusion_matrix = torch.add(total_confusion_matrix, confusion_matrix)\n",
    "        output_dict_in[equity].append([test_start,predictions_ord])\n",
    "        model_dict_in[equity].append([test_start,model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintConfMatrix(total_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mat(total_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime('2002-01-01')\n",
    "end_date = pd.to_datetime('2006-12-31')\n",
    "output_dict_in = {}\n",
    "for equity in equity_list:\n",
    "    output_dict_in[equity] = []\n",
    "    \n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    start = start_date +pd.offsets.DateOffset(years=i)\n",
    "    end = end_date + pd.offsets.DateOffset(years=i)\n",
    "    test_start = start_date +pd.offsets.DateOffset(years=(i+5))\n",
    "    test_end = end_date +pd.offsets.DateOffset(years=(i+1))\n",
    "    gen_training_data(equities_dfs,equity_list,start,end,feature_list,training_directory)\n",
    "    gen_training_data(equities_dfs,equity_list,test_start,test_end,feature_list,test_directory, test = True, ordered_dir = ordered_directory)    \n",
    "    \n",
    "    for equity in equity_list:\n",
    "        _, _, _, _, _, test_dataloader_ordered,_ = get_loaders(equity)\n",
    "        _, _, predictions_ord = test_model(model_dict_in[equity][i][1], test_dataloader_ordered)\n",
    "        output_dict_in[equity].append([model_dict_in[equity][i][0],predictions_ord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for equity in equity_list:\n",
    "    eq_df = pd.DataFrame()\n",
    "    for preds in output_dict_in[equity]:\n",
    "        if eq_df.empty == True:\n",
    "            eq_df = pd.DataFrame({str(preds[0]):preds[1].tolist()})\n",
    "        else:\n",
    "            temp_df = pd.DataFrame({str(preds[0]):preds[1].tolist()})\n",
    "            eq_df = pd.concat([eq_df, temp_df],axis = 1)\n",
    "    eq_df.to_csv(equity+'_inception.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
